#!/usr/bin/env python3
"""
Test the fine-tuned Khmer medical model
"""

import torch
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel

def parse_args():
    parser = argparse.ArgumentParser(description="Test fine-tuned Khmer medical model")
    parser.add_argument("--model_path", required=True, help="Path to fine-tuned model")
    parser.add_argument("--base_model", default=None, help="Base model if using LoRA")
    parser.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--max_length", type=int, default=512)
    parser.add_argument("--temperature", type=float, default=0.7)
    parser.add_argument("--interactive", action="store_true", help="Interactive mode")
    return parser.parse_args()

def load_model(args):
    """Load the fine-tuned model"""
    print(f"ğŸ”„ Loading model from: {args.model_path}")
    
    # Check if it's a LoRA model
    if args.base_model:
        # Load base model
        model = AutoModelForCausalLM.from_pretrained(
            args.base_model,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        # Load LoRA weights
        model = PeftModel.from_pretrained(model, args.model_path)
        model = model.merge_and_unload()
    else:
        # Load full model
        model = AutoModelForCausalLM.from_pretrained(
            args.model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    
    return model, tokenizer

def test_questions():
    """Sample medical questions in Khmer"""
    return [
        "áá¾á‡áŸ†á„áºá‘á¹á€á“áŸ„á˜á•áŸ’á¢áŸ‚á˜á”áŸ’ášá—áŸá‘á‘á¸áŸ¢ á˜á¶á“ášáŸ„á‚áŸá‰áŸ’á‰á¶á¢áŸ’áœá¸ááŸ’á›áŸ‡?",
        "áá¾á‚á½ášá’áŸ’áœá¾áŠá¼á…á˜áŸ’ááŸá… á”áŸ’ášáŸá·á“á”á¾á˜á¶á“á‚áŸ’ášá»á“á€áŸ’ááŸ…ááŸ’á›á¶áŸ†á„?",
        "áá¾á¢á¶á á¶ášá¢áŸ’áœá¸ááŸ’á›áŸ‡áŠáŸ‚á›á›áŸ’á¢áŸá˜áŸ’ášá¶á”áŸ‹á¢áŸ’á“á€á˜á¶á“á‡áŸ†á„áºá›á¾áŸáˆá¶á˜?",
        "áá¾á€á¶ášá á¶ááŸ‹á”áŸ’ášá¶áá”áŸ‰á»á“áŸ’á˜á¶á“áŠá„á€áŸ’á“á»á„á˜á½á™áŸá”áŸ’áá¶á áŸá‚áºá›áŸ’á¢áŸá˜áŸ’ášá¶á”áŸ‹áŸá»áá—á¶á–?",
        "áá¾á˜á¶á“áœá·á’á¸á¢áŸ’áœá¸ááŸ’á›áŸ‡áŠá¾á˜áŸ’á”á¸á€á¶ášá–á¶ášá‡áŸ†á„áºá‚áŸ’ášá»á“á…á¶á‰áŸ‹?",
    ]

def format_prompt(question, model_type="qwen"):
    """Format prompt based on model type"""
    if "qwen" in model_type.lower():
        return f"""<|im_start|>system
á¢áŸ’á“á€á‚áºá‡á¶á‡áŸ†á“á½á™á€á¶ášáœáŸá‡áŸ’á‡áŸá¶áŸáŸ’ááŸ’ášáŠáŸ‚á›á˜á¶á“á…áŸ†ááŸáŸ‡áŠá¹á„áŸ”<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
"""
    else:  # ChatML format
        return f"""<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
"""

def run_inference(model, tokenizer, question, args):
    """Run inference on a single question"""
    # Determine model type
    model_type = "qwen" if "qwen" in args.model_path.lower() else "chatml"
    
    # Format prompt
    prompt = format_prompt(question, model_type)
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=args.max_length,
            temperature=args.temperature,
            do_sample=True,
            top_p=0.95,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # Decode
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract only the assistant's response
    if "<|im_start|>assistant" in response:
        response = response.split("<|im_start|>assistant")[-1].strip()
    
    return response

def interactive_mode(model, tokenizer, args):
    """Interactive chat mode"""
    print("\nğŸ’¬ Interactive Mode (type 'quit' to exit)")
    print("=" * 50)
    
    while True:
        question = input("\nğŸ‘¤ áŸáŸ†áá½áš (Question): ").strip()
        
        if question.lower() in ['quit', 'exit', 'bye']:
            print("ğŸ‘‹ á›á¶áŸá·á“á á¾á™! (Goodbye!)")
            break
        
        if not question:
            continue
        
        print("\nğŸ¤– á…á˜áŸ’á›á¾á™ (Answer):")
        response = run_inference(model, tokenizer, question, args)
        print(response)

def main():
    args = parse_args()
    
    # Load model
    model, tokenizer = load_model(args)
    
    if args.interactive:
        # Interactive mode
        interactive_mode(model, tokenizer, args)
    else:
        # Test with sample questions
        print("\nğŸ§ª Testing with sample medical questions...")
        print("=" * 50)
        
        for i, question in enumerate(test_questions(), 1):
            print(f"\nğŸ“ Question {i}:")
            print(f"   {question}")
            print(f"\nğŸ’Š Answer:")
            response = run_inference(model, tokenizer, question, args)
            print(f"   {response}")
            print("-" * 50)
        
        print("\nâœ… Testing complete!")
        
        # Offer interactive mode
        print("\nğŸ’¡ Tip: Run with --interactive for chat mode")

if __name__ == "__main__":
    main()