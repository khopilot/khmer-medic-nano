{
  "dataset_info": {
    "name": "khmer-medical-qa",
    "version": "1.0",
    "language": "km",
    "domain": "medical",
    "total_examples": 18756,
    "with_paraphrases": 9314,
    "with_reasoning": 18753
  },
  "formats_available": [
    "alpaca",
    "chatml",
    "llama",
    "qwen",
    "supervised"
  ],
  "statistics": {
    "alpaca": {
      "train": 26667,
      "val": 1403,
      "total": 28070
    },
    "chatml": {
      "train": 26667,
      "val": 1403,
      "total": 28070
    },
    "llama": {
      "train": 26667,
      "val": 1403,
      "total": 28070
    },
    "qwen": {
      "train": 26667,
      "val": 1403,
      "total": 28070
    },
    "supervised": {
      "train": 26667,
      "val": 1403,
      "total": 28070
    }
  },
  "recommended_settings": {
    "qwen_2.5_1.5b": {
      "format": "qwen",
      "learning_rate": 2e-05,
      "batch_size": 4,
      "gradient_accumulation": 8,
      "epochs": 3,
      "warmup_ratio": 0.1,
      "max_length": 2048,
      "lora_r": 32,
      "lora_alpha": 64
    },
    "llama_2_7b": {
      "format": "llama",
      "learning_rate": 1e-05,
      "batch_size": 2,
      "gradient_accumulation": 16,
      "epochs": 3,
      "warmup_ratio": 0.05,
      "max_length": 2048,
      "lora_r": 16,
      "lora_alpha": 32
    },
    "smollm_1.7b": {
      "format": "chatml",
      "learning_rate": 3e-05,
      "batch_size": 8,
      "gradient_accumulation": 4,
      "epochs": 4,
      "warmup_ratio": 0.1,
      "max_length": 1536,
      "lora_r": 32,
      "lora_alpha": 64
    }
  }
}